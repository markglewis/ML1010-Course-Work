{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data - Natural Language Processing (NLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================\n",
    "# Part A.  Introductory Materials\n",
    "# ===================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data usually consists of a collection of documents (called the corpus) which can represent words, sentences, or even paragraphs of free flowing text. \n",
    "\n",
    "The inherent unstructured (no neatly formatted data columns!) and noisy nature of textual data makes it harder for machine learning methods to directly work on raw text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering dramatically improve performance of machine learning models and wins Kaggle competitions.  This is especially true for text data, which is unstructured, noisy, and complex.  \n",
    "\n",
    "This section will cover the following types of features for text data\n",
    "\n",
    "1.  Bag of Words\n",
    "\n",
    "2.  Bag of N-Grams (uni-gram, bi-gram, tri-gram, etc.)\n",
    "\n",
    "3.  TF-IDF (term frequency over inverse document frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample \"corpus\" of documents:  the Document contains short sentences and each text belongs to a category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category\n",
       "0                     The sky is blue and beautiful.  weather\n",
       "1                  Love this blue and beautiful sky!  weather\n",
       "2       The quick brown fox jumps over the lazy dog.  animals\n",
       "3   The brown fox is quick and the blue dog is lazy!  animals\n",
       "4  The sky is very blue and the sky is very beaut...  weather\n",
       "5        The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "\n",
    "labels = ['weather', 'weather', 'animals', 'animals', 'weather', 'animals']\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your downstream task, cleaning and pre-processing text can involve several different components.  Here are a few important components of Natural Language Processing (NLP) pipelines.\n",
    "\n",
    "1.  Removing tags: unnecessary content like HTML tags\n",
    "\n",
    "2.  Removing accented characters: other languages such as French, convert ASCII\n",
    "\n",
    "3. Removing special characters: adds noise to text, use simple regular expressions (regexes)\n",
    "\n",
    "4.  Stemming and lemmatization: Stemming remove prefixes and suffixes of word stems (i.e. root words), ex. WATCH is the root stem of WATCHES, WATCHING, and WATCHE.  Lemmatization similar but lexicographically correct word (present in the dictionary).\n",
    "\n",
    "5.  Expanding contractions: helps text standardization, ex. do not to don’t and I would to I’d\n",
    "\n",
    "6.  Removing stopwords: Words without meaningful significance (ex. a, an, the, and) but high frequency.\n",
    "\n",
    "Additional pre-processing: tokenization, removing extra whitespaces,  lower casing and more advanced operations like spelling corrections, grammatical error corrections, removing repeated characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog', 'brown fox quick blue dog lazy',\n",
       "       'sky blue sky beautiful today', 'dog lazy brown fox quick'],\n",
       "      dtype='<U30')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Bag of Words Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perhaps the most simple vector space representational model for unstructured text. A vector space model is simply a mathematical model to represent unstructured text (or any other data) as numeric vectors, such that each dimension of the vector is a specific feature\\attribute. The bag of words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0) or even weighted values. The model’s name is such because each document is represented literally as a ‘bag’ of its own words, disregarding word orders, sequences and grammar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus you can see that our documents have been converted into numeric vectors such that each document is represented by one vector (row) in the above feature matrix. The following code will help represent this in a more easy to understand format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sky</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beautiful  blue  brown  dog  fox  jumps  lazy  love  quick  sky  today\n",
       "0          1     1      0    0    0      0     0     0      0    1      0\n",
       "1          1     1      0    0    0      0     0     1      0    1      0\n",
       "2          0     0      1    1    1      1     1     0      1    0      0\n",
       "3          0     1      1    1    1      0     1     0      1    0      0\n",
       "4          1     1      0    0    0      0     0     0      0    2      1\n",
       "5          0     0      1    1    1      0     1     0      1    0      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should make things more clearer! You can clearly see that each column or dimension in the feature vectors represents a word from the corpus and each row represents one of our documents. The value in any cell, represents the number of times that word (represented by column) occurs in the specific document (represented by row). Hence if a corpus of documents consists of N unique words across all the documents, we would have an N-dimensional vector for each of the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should make things more clearer! You can clearly see that each column or dimension in the feature vectors represents a word from the corpus and each row represents one of our documents. The value in any cell, represents the number of times that word (represented by column) occurs in the specific document (represented by row). Hence if a corpus of documents consists of N unique words across all the documents, we would have an N-dimensional vector for each of the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Bag of N-Grams Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word is just a single token, often known as a unigram or 1-gram. We already know that the Bag of Words model doesn’t consider order of words. But what if we also wanted to take into account phrases or collection of words which occur in a sequence? N-grams help us achieve that. An N-gram is basically a collection of word tokens from a text document such that these tokens are contiguous and occur in a sequence. Bi-grams indicate n-grams of order 2 (two words), Tri-grams indicate n-grams of order 3 (three words), and so on. The Bag of N-Grams model is hence just an extension of the Bag of Words model so we can also leverage N-gram based features. The following example depicts bi-gram based features in each document feature vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>beautiful today</th>\n",
       "      <th>blue beautiful</th>\n",
       "      <th>blue dog</th>\n",
       "      <th>blue sky</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog lazy</th>\n",
       "      <th>fox jumps</th>\n",
       "      <th>fox quick</th>\n",
       "      <th>jumps lazy</th>\n",
       "      <th>lazy brown</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>love blue</th>\n",
       "      <th>quick blue</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>sky beautiful</th>\n",
       "      <th>sky blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beautiful sky  beautiful today  blue beautiful  blue dog  blue sky  \\\n",
       "0              0                0               1         0         0   \n",
       "1              1                0               1         0         0   \n",
       "2              0                0               0         0         0   \n",
       "3              0                0               0         1         0   \n",
       "4              0                1               0         0         1   \n",
       "5              0                0               0         0         0   \n",
       "\n",
       "   brown fox  dog lazy  fox jumps  fox quick  jumps lazy  lazy brown  \\\n",
       "0          0         0          0          0           0           0   \n",
       "1          0         0          0          0           0           0   \n",
       "2          1         0          1          0           1           0   \n",
       "3          1         1          0          1           0           0   \n",
       "4          0         0          0          0           0           0   \n",
       "5          1         1          0          1           0           1   \n",
       "\n",
       "   lazy dog  love blue  quick blue  quick brown  sky beautiful  sky blue  \n",
       "0         0          0           0            0              0         1  \n",
       "1         0          1           0            0              0         0  \n",
       "2         1          0           0            1              0         0  \n",
       "3         0          0           1            0              0         0  \n",
       "4         0          0           0            0              1         1  \n",
       "5         0          0           0            0              0         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us feature vectors for our documents, where each feature consists of a bi-gram representing a sequence of two words and values represent how many times the bi-gram was present for our documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  TF-IDF Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some potential problems which might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms which occur frequently across all documents and these may tend to overshadow other terms in the feature set. The TF-IDF model tries to combat this issue by using a scaling or normalizing factor in its computation. TF-IDF stands for Term Frequency-Inverse Document Frequency, which uses a combination of two metrics in\n",
    "its computation, namely: term frequency (tf) and inverse document frequency (idf). This technique was developed for ranking results for queries in search engines and now it is an indispensable model in the world of information retrieval and NLP.\n",
    "\n",
    "Mathematically, we can define TF-IDF as tfidf = tf x idf, which can be expanded further to be represented as follows.\n",
    "\n",
    "\n",
    "Here, tfidf(w, D) is the TF-IDF score for word w in document D. The term tf(w, D) represents the term frequency of the word w in document D, which can be obtained from the Bag of Words model. The term idf(w, D) is the inverse document frequency for the term w, which can be computed as the log transform of the total number of documents in the corpus C divided by the document frequency of the word w, which is basically the frequency of documents in the corpus where the word w occurs. There are multiple variants of this model but they all end up giving quite similar results. Let’s apply this on our corpus now!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sky</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beautiful  blue  brown   dog   fox  jumps  lazy  love  quick   sky  today\n",
       "0       0.60  0.52   0.00  0.00  0.00   0.00  0.00  0.00   0.00  0.60   0.00\n",
       "1       0.46  0.39   0.00  0.00  0.00   0.00  0.00  0.66   0.00  0.46   0.00\n",
       "2       0.00  0.00   0.38  0.38  0.38   0.54  0.38  0.00   0.38  0.00   0.00\n",
       "3       0.00  0.36   0.42  0.42  0.42   0.00  0.42  0.00   0.42  0.00   0.00\n",
       "4       0.36  0.31   0.00  0.00  0.00   0.00  0.00  0.00   0.00  0.72   0.52\n",
       "5       0.00  0.00   0.45  0.45  0.45   0.00  0.45  0.00   0.45  0.00   0.00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF based feature vectors for each of our text documents show scaled and normalized values as compared to the raw Bag of Words model values. Interested readers who might want to dive into further details of how the internals of this model work can refer to page 181 of Text Analytics with Python (Springer\\Apress; Dipanjan Sarkar, 2016).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================\n",
    "# Part B.  Intermediate Materials\n",
    "# ===================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still time left?  Let's cover some more advanced clustering techniques:\n",
    "\n",
    "1.  Document Clustering with Similarity Features\n",
    "2.  Topic Models\n",
    "3.  Document Similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Document Similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document similarity is the process of using a distance or similarity based metric that can be used to identify how similar a text document is with any other document(s) based on features extracted from the documents like bag of words or tf-idf.\n",
    "Thus you can see that we can build on top of the tf-idf based features we engineered in the previous section and use them to generate new features which can be useful in domains like search engines, document clustering and information retrieval by leveraging these similarity based features.\n",
    "\n",
    "Pairwise document similarity in a corpus involves computing document similarity for each pair of documents in a corpus. Thus if you have C documents in a corpus, you would end up with a C x C matrix such that each row and column represents the similarity score for a pair of documents, which represent the indices at the row and column, respectively. There are several similarity and distance metrics that are used to compute document similarity. These include cosine distance/similarity, euclidean distance, manhattan distance, BM25 similarity, jaccard distance and so on. In our analysis, we will be using perhaps the most popular and widely used similarity metric,\n",
    "cosine similarity and compare pairwise document similarity based on their TF-IDF feature vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.753128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.807539</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.753128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.608181</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.784362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109653</td>\n",
       "      <td>0.933779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.807539</td>\n",
       "      <td>0.608181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109653</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839987</td>\n",
       "      <td>0.933779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  1.000000  0.753128  0.000000  0.185447  0.807539  0.000000\n",
       "1  0.753128  1.000000  0.000000  0.139665  0.608181  0.000000\n",
       "2  0.000000  0.000000  1.000000  0.784362  0.000000  0.839987\n",
       "3  0.185447  0.139665  0.784362  1.000000  0.109653  0.933779\n",
       "4  0.807539  0.608181  0.000000  0.109653  1.000000  0.000000\n",
       "5  0.000000  0.000000  0.839987  0.933779  0.000000  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity basically gives us a metric representing the cosine of the angle between the feature vector representations of two text documents. Lower the angle between the documents, the closer and more similar they are as depicted in the following figure.\n",
    "\n",
    "\n",
    "Cosine similarity depictions for text document feature vectors\n",
    "Looking closely at the similarity matrix clearly tells us that documents (0, 1 and 6), (2, 5 and 7) are very similar to one another and documents 3 and 4 are slightly similar to each other but the magnitude is not very strong, however still stronger than the other documents. This must indicate these similar documents have some similar features. This is a perfect example of grouping or clustering that can be solved by unsupervised learning especially when you are dealing with huge corpora of millions of text documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Document Clustering with Similarity Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering leverages unsupervised learning to group data points (documents in this scenario) into groups or clusters. We will be leveraging an unsupervised hierarchical clustering algorithm here to try and group similar documents from our toy corpus together by leveraging the document similarity features we generated earlier. There are two types of hierarchical clustering algorithms namely, agglomerative and divisive methods. We will be using a agglomerative clustering algorithm, which is hierarchical clustering using a bottom up approach i.e. each observation or document starts in its own cluster and clusters are successively merged together using a distance metric which measures distances between data points and a linkage merge criterion. A sample depiction is shown in the following figure.\n",
    "\n",
    "\n",
    "The selection of the linkage criterion governs the merge strategy. Some examples of linkage criteria are Ward, Complete linkage, Average linkage and so on. This criterion is very useful for choosing the pair of clusters (individual documents at the lowest step and clusters in higher steps) to merge at each step is based on the optimal value of an objective function. We choose the Ward’s minimum variance method as our linkage criterion to minimize total within-cluster variance. Hence, at each step, we find the pair of clusters that leads to minimum increase in total within-cluster variance after merging. Since we already have our similarity features, let’s build out the linkage matrix on our sample documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document\\Cluster 1</th>\n",
       "      <th>Document\\Cluster 2</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Cluster Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.271171</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.317548</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.373037</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.531801</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3.44916</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document\\Cluster 1 Document\\Cluster 2  Distance Cluster Size\n",
       "0                  2                  5  0.271171            2\n",
       "1                  0                  4  0.317548            2\n",
       "2                  3                  6  0.373037            3\n",
       "3                  1                  7  0.531801            3\n",
       "4                  8                  9   3.44916            6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(similarity_matrix, 'ward')\n",
    "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n",
    "                         'Distance', 'Cluster Size'], dtype='object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you closely look at the linkage matrix, you can see that each step (row) of the linkage matrix tells us which data points (or clusters) were merged together. If you have n data points, the linkage matrix, Z will be having a shape of (n — 1) x 4 where Z[i] will tell us which clusters were merged at step i. Each row has four elements, the first two elements are either data point identifiers or cluster labels (in the later parts of the matrix once\n",
    "multiple data points are merged), the third element is the cluster distance between the first two elements (either data points or clusters), and the last element is the total number of elements\\data points in the cluster once the merge is complete. We recommend you refer to the scipy documentation, which explains this in detail.\n",
    "\n",
    "Let’s now visualize this matrix as a dendrogram to understand the elements better!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x21c19923438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Data point')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=1.0, c='k', ls='--', lw=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models\n",
    "We can also use some summarization techniques to extract topic or concept based features from text documents. The idea of topic models revolves around the process of extracting key themes or concepts from a corpus of documents which are represented as topics. Each topic can be represented as a bag or collection of words/terms from the document corpus. Together, these terms signify a specific topic, theme or a concept and each topic can be easily distinguished from other topics by virtue of the semantic meaning conveyed by these terms. However often you do end up with overlapping topics based on the data. These concepts can range from simple facts and statements to opinions and outlook. Topic models are extremely useful in summarizing large corpus of text documents to extract and depict key concepts. They are also useful in extracting features from text data that capture latent patterns in the data.\n",
    "\n",
    "There are various techniques for topic modeling and most of them involve some form of matrix decomposition. Some techniques like Latent Semantic Indexing (LSI) use matrix decomposition operations, more specifically Singular Valued Decomposition. We will be using another technique is Latent Dirichlet Allocation (LDA), which uses a generative probabilistic model where each document consists of a combination of several topics and each term or word can be assigned to a specific topic. This is similar to pLSI based model (probabilistic LSI). Each latent topic contains a Dirichlet prior over them in the case of LDA.\n",
    "\n",
    "The math behind in this technique is pretty involved, so I will try to summarize it without boring you with a lot of details. I recommend readers to go through this excellent talk by Christine Doig.\n",
    "\n",
    "The black box in the above figure represents the core algorithm that makes use of the previously mentioned parameters to extract K topics from M documents. The following steps give a simplistic explanation of what happens in the algorithm behind the scenes.\n",
    "\n",
    "Once this runs for several iterations, we should have topic mixtures for each document and then generate the constituents of each topic from the terms that point to that topic. Frameworks like gensim or scikit-learn enable us to leverage the LDA model for generating topics.\n",
    "\n",
    "For the purpose of feature engineering which is the intent of this article, you need to remember that when LDA is applied on a document-term matrix (TF-IDF or Bag of Words feature matrix), it gets decomposed into two main components.\n",
    "\n",
    "A document-topic matrix, which would be the feature matrix we\n",
    "are looking for.\n",
    "A topic-term matrix, which helps us in looking at potential topics in the corpus.\n",
    "Let’s leverage scikit-learn to get the document-topic matrix as follows.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mglewis\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831570</td>\n",
       "      <td>0.084281</td>\n",
       "      <td>0.084149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.864945</td>\n",
       "      <td>0.067312</td>\n",
       "      <td>0.067743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047801</td>\n",
       "      <td>0.903651</td>\n",
       "      <td>0.048548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055404</td>\n",
       "      <td>0.896033</td>\n",
       "      <td>0.048563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.887660</td>\n",
       "      <td>0.055993</td>\n",
       "      <td>0.056347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.055710</td>\n",
       "      <td>0.887959</td>\n",
       "      <td>0.056331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         T1        T2        T3\n",
       "0  0.831570  0.084281  0.084149\n",
       "1  0.864945  0.067312  0.067743\n",
       "2  0.047801  0.903651  0.048548\n",
       "3  0.055404  0.896033  0.048563\n",
       "4  0.887660  0.055993  0.056347\n",
       "5  0.055710  0.887959  0.056331"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=10000, random_state=0)\n",
    "dt_matrix = lda.fit_transform(cv_matrix)\n",
    "features = pd.DataFrame(dt_matrix, columns=['T1', 'T2', 'T3'])\n",
    "features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see which documents contribute the most to which of the three topics in the above output. You can view the topics and their main constituents as follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sky', 4.330354268539671), ('blue', 3.3755375835723727), ('beautiful', 3.3301184041393532), ('today', 1.3307001091581214), ('love', 1.329974958258021)]\n",
      "\n",
      "[('brown', 3.3302369156308287), ('dog', 3.3302369156308287), ('fox', 3.3302369156308287), ('lazy', 3.3302369156308287), ('quick', 3.3302369156308287), ('jumps', 1.330279231916799), ('blue', 1.285679256293642)]\n",
      "\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt_matrix = lda.components_\n",
    "for topic_weights in tt_matrix:\n",
    "    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n",
    "    topic = sorted(topic, key=lambda x: -x[1])\n",
    "    topic = [item for item in topic if item[1] > 0.6]\n",
    "    print(topic)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering with Topic Model Features\n",
    "We used our Bag of Words model based features to build out topic model based features using LDA. We can now actually leverage the document term matrix we obtained and use an unsupervised clustering algorithm to try and group our documents similar to what we did earlier with our similarity features.\n",
    "\n",
    "We will use a very popular partition based clustering method this time, K-means clustering to cluster or group these documents based on their topic model feature representations. In K-means clustering, we have an input parameter k, which specifies the number of clusters it will output using the document features. This clustering method is a centroid based clustering method, where it tries to cluster these documents into clusters of equal variance. It tries to create these clusters by minimizing the within-cluster sum of squares measure, also known as inertia. There are multiple ways to select the optimal value of k like using the Sum of Squared Errors metric, Silhouette Coefficients and the Elbow method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category  ClusterLabel\n",
       "0                     The sky is blue and beautiful.  weather             2\n",
       "1                  Love this blue and beautiful sky!  weather             0\n",
       "2       The quick brown fox jumps over the lazy dog.  animals             1\n",
       "3   The brown fox is quick and the blue dog is lazy!  animals             1\n",
       "4  The sky is very blue and the sky is very beaut...  weather             0\n",
       "5        The dog is lazy but the brown fox is quick!  animals             1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "km.fit_transform(features)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================\n",
    "# Part C.  Advanced Materials\n",
    "# ===================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done and have more time?  Help your classmates with completing their exercise or try the next tutorial on word embeddings as features.\n",
    "\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortcomings of traditional models:\n",
    "\n",
    "Traditional (count-based) feature engineering strategies for textual data involve models belonging to a family of models popularly known as the Bag of Words model. This includes term frequencies, TF-IDF (term frequency-inverse document frequency), N-grams and so on. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence and context around nearby words in each text document. This forms as enough motivation for us to explore more sophisticated models which can capture this information and give us features which are vector representation of words, popularly known as embeddings.\n",
    "\n",
    "# The need for word embeddings:\n",
    "\n",
    "While this does make some sense, why should we be motivated enough to learn and build these word embeddings? With regard to speech or image recognition systems, all the information is already present in the form of rich dense feature vectors embedded in high-dimensional datasets like audio spectrograms and image pixel intensities. However when it comes to raw text data, especially count based models like Bag of Words, we are dealing with individual words which may have their own identifiers and do not capture the semantic relationship amongst words. This leads to huge sparse word vectors for textual data and thus if we do not have enough data, we may end up getting poor models or even overfitting the data due to the curse of dimensionality.\n",
    "\n",
    "To overcome the shortcomings of losing out semantics and feature sparsity in bag of words model based features, we need to make use of Vector Space Models (VSMs) in such a way that we can embed word vectors in this continuous vector space based on semantic and contextual similarity. In fact the distributional hypothesis in the field of distributional semantics tells us that words which occur and are used in the same context, are semantically similar to one another and have similar meanings. In simple terms, ‘a word is characterized by the company it keeps’. One of the famous papers talking about these semantic word vectors and various types in detail is ‘Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors’ by Baroni et al. We won’t go into extensive depth but in short, there are two main types of methods for contextual word vectors. Count-based methods like Latent Semantic Analysis (LSA) which can be used to compute some statistical measures of how often words occur with their neighboring words in a corpus and then building out dense word vectors for each word from these measures. Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process it learns distributed representations giving us dense word embeddings. We will be focusing on these predictive methods in this article.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
